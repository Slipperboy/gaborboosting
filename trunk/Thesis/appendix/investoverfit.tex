\chapter{Investigation of Overfitting}
\label{apx:overfitting}
AdaBoost has its potential to overfit the training set because its objective is to minimise error on the training set \cite{Bylander2006}. At each iteration, AdaBoost focuses on classifying the mis-classified examples, which might result in fitting noises during the training. Since the AdaBoost classifier is a combination of many weak learners, the classification results demonstrate the overall performance among these weak learners. Ideally, without considering overfitting, more weak learners would lead a better classification performance. With more iterations, classification performace would be improved. However, more weak learners may bring the loss on the classification accuracy, because these learners trend to fit noise better than to fit the data itself. The reason of overfitting is investigated by exploring false negative $\mathcal{FN}$ and false positive $\mathcal{FP}$ on each iteration. \mbox{Table} \ref{tab:ovefitting} shows the performance of the training and testing dataset on each iteration with corresponding false negative $\mathcal{FN}$ and false positive $\mathcal{FP}$.
\begin{table}
\caption{The performance of each iteration in the training and testing set by the AdaBoost classifier.}
\begin{footnotesize}
\subtable[The 1st client]{
 \begin{tabular}{|c|c|c|c|c|}
  \hline
   & \multicolumn{2}{c|}{Training Set (\%)} & \multicolumn{2}{c|}{Testing Set (\%)}\\
   \hline
  Iter & $\mathcal{FN}$ & $\mathcal{FP}$ & $\mathcal{FN}$ & $\mathcal{FP}$ \\
  \hline
  1 & 0.0 & 1.13 & 75 & 0.77 \\
  2 & 0.0 & 9 & 3 & 0.77 \\
  3 & 0.0 & 1.13 & 75 & 0.25 \\
  4 & 0.0 & 0.0 & 25 & 0.19 \\
  5 & 0.0 & 0.0 & 50 & 0.13 \\
  6 & 0.0 & 0.0 & 75 & 0.0 \\
  7 & 0.0 & 0.0 & 100 & 0.0 \\
 \hline
\end{tabular}
}
\subtable[The 2nd client]{
 \begin{tabular}{|c|c|c|c|c|}
 \hline
  & \multicolumn{2}{c|}{Training Set (\%)} & \multicolumn{2}{c|}{Testing Set (\%)}\\
   \hline
  Iter & $\mathcal{FN}$ & $\mathcal{FP}$ & $\mathcal{FN}$ & $\mathcal{FP}$ \\
  \hline
  1 & 0.0 & 1.0 & 100 & 0.19 \\
  2 & 0.0 & 1.0 & 100 & 1.6 \\
  3 & 0.0 & 0.0 & 100 & 0.0 \\
  4 & 0.0 & 0.0 & 100 & 0.0 \\
  5 & 0.0 & 0.0 & 100 & 0.0 \\
  6 & 0.0 & 0.0 & 100 & 0.0 \\
  \hline
\end{tabular}
}
\subtable[The 3rd client]{
 \begin{tabular}{|c|c|c|c|c|}
  \hline
   & \multicolumn{2}{c|}{Training Set (\%)} & \multicolumn{2}{c|}{Testing Set (\%)}\\
   \hline
  Iter & $\mathcal{FN}$ & $\mathcal{FP}$ & $\mathcal{FN}$ & $\mathcal{FP}$ \\
  \hline
  1 & 0.0 & 0.5 & 100 & 0.58 \\
  2 & 0.0 & 0.5 & 100 & 0.58 \\
  3 & 0.0 & 0.0 & 75 & 0.25 \\
  4 & 0.0 & 0.0 & 100 & 0.0 \\
  5 & 0.0 & 0.0 & 100 & 0.0 \\
  \hline
\end{tabular}
}
\subtable[The 4th client]{
 \begin{tabular}{|c|c|c|c|c|}
  \hline
   & \multicolumn{2}{c|}{Training Set (\%} & \multicolumn{2}{c|}{Testing Set (\%)}\\
   \hline
  Iter & $\mathcal{FN}$ & $\mathcal{FP}$ & $\mathcal{FN}$ & $\mathcal{FP}$ \\
  \hline
  1 & 0.0 & 0.13 & 25 & 0.96 \\
  2 & 0.0 & 0.13 & 25 & 0.96 \\
  3 & 0.0 & 0.0 & 25 & 0.13 \\
  4 & 0.0 & 0.0 & 25 & 0.19 \\
  5 & 0.0 & 0.0 & 75 & 0.0 \\
  6 & 0.0 & 0.0 & 75 & 0.0 \\
  7 & 0.0 & 0.0 & 100 & 0.0 \\
  \hline
 \end{tabular}
}
\subtable[The 5th client]{
 \begin{tabular}{|c|c|c|c|c|}
  \hline
   & \multicolumn{2}{c|}{Training Set (\%)} & \multicolumn{2}{c|}{Testing Set (\%)}\\
   \hline
  Iter & $\mathcal{FN}$ & $\mathcal{FP}$ & $\mathcal{FN}$ & $\mathcal{FP}$ \\
  \hline
  1 & 0.0 & 1.5 & 75 & 2.3 \\
  2 & 0.0 & 1.6 & 25 & 3.5 \\
  3 & 0.0 & 0.0 & 25 & 0.39 \\
  4 & 0.0 & 0.0 & 25 & 0.39 \\
  5 & 0.0 & 0.0 & 25 & 0.0 \\
  6 & 0.0 & 0.0 & 25 & 0.0 \\
  7 & 0.0 & 0.0 & 50 & 0.0 \\
  8 & 0.0 & 0.0 & 50 & 0.0 \\
  9 & 0.0 & 0.0 & 75 & 0.0 \\
  \hline
 \end{tabular}
}
\subtable[The 6th client]{
 \begin{tabular}{|c|c|c|c|c|}
  \hline
   & \multicolumn{2}{c|}{Training Set (\%)} & \multicolumn{2}{c|}{Testing Set (\%)}\\
   \hline
  Iter & $\mathcal{FN}$ &$\mathcal{FP}$ & $\mathcal{FN}$ & $\mathcal{FP}$ \\
  \hline
  1 & 0.0 & 0.25 & 25 & 0.51 \\
  2 & 0.0 & 0.25 & 25 & 0.51 \\
  3 & 0.0 & 0.0 & 100 & 0.0 \\
  \hline
 \end{tabular}
}
\subtable[The 7th client]{
 \begin{tabular}{|c|c|c|c|c|}
  \hline
   & \multicolumn{2}{c|}{Training Set (\%)} & \multicolumn{2}{c|}{Testing Set (\%)}\\
   \hline
  Iter & $\mathcal{FN}$ & $\mathcal{FP}$ & $\mathcal{FN}$ & $\mathcal{FP}$ \\
  \hline
  1 & 0.0 & 0.50 & 100 & 0.51 \\
  2 & 0.0 & 0.50 & 100 & 0.51 \\
  3 & 0.0 & 0.0 & 100 & 0.13 \\
  4 & 0.0 & 0.0 & 100 & 0.96 \\
  5 & 0.0 & 0.0 & 100 & 0.13 \\
  6 & 0.0 & 0.0 & 100 & 0.0 \\
  7 & 0.0 & 0.0 & 100 & 0.13 \\
  8 & 0.0 & 0.0 & 100 & 0.0 \\
  \hline
 \end{tabular}
}
\subtable[The 8th client]{
  \begin{tabular}{|c|c|c|c|c|}
  \hline
   & \multicolumn{2}{c|}{Training Set (\%)} & \multicolumn{2}{c|}{Testing Set (\%)}\\
   \hline
  Iter & $\mathcal{FN}$ & $\mathcal{FP}$ & $\mathcal{FN}$ & $\mathcal{FP}$ \\
  \hline
  1 & 0.0 & 0.25 & 75 & 1.3 \\
  2 & 0.0 & 0.25 & 100 & 0.45 \\
  3 & 0.0 & 0.0 & 75 & 0.0 \\
  4 & 0.0 & 0.0 & 100 & 0.0 \\
  \hline
 \end{tabular}
}
\end{footnotesize}
\label{tab:ovefitting}
\end{table} 
For all eight clients, the number of false negative $\mathcal{FN}$ in the training set are zero after the first and second iterations. It means that after the training of the first two iterations, all positive examples in the training set have been classified correctly, and there are some negative examples which are mis-classified as the positive. In the first two iterations, the classifier has more focused on the positive examples. The classifier does not concern on how many negative examples are misclassified, such that the false positive rate is the highest among all iterations. At the beginning of AdaBoost training, the weights are initialised according to number of examples involved. Hence, the weights on positive examples are much higher than negative examples ($\frac{1}{8}$ v.s. $\frac{1}{1592}$). At the first two iterations, the AdaBoost training is more focused on the positive examples, which means the training is trying to classify the positive examples correctly, but regardless to the negative examples. After these two iterations, the weights on those mis-classified negative examples are increased, but the weights on the positive examples are decreased. After the third iterations, the false positive rate has dropped to zero, because the AdaBoost training has changed to focus on those negative examples. Generally, AdaBoost focuses on the examples with higher weights. When these examples are classified, the weights will be updated across the train set, and the AdaBoost training shifts its focus onto other examples. In this case, AdaBoost firstly focuses on positive examples, then to add more constraints on negative examples and to achieve zero error in the training set. Consequently, the mechanism leads the overfitting on the training set.

From \mbox{Table} \ref{tab:ovefitting}, it is seen that the performance of AdaBoost classifier gets stable only after three iterations. When the training is going on further more than three iterations, the overall error (false negative and false positive) is still kept as zero. More iterations means more selected features,\textit{ i.e.}, more weak learners are combined into the AdaBoost classifier $H$. Finally, the performance on the testing set all ends at a point of $100\%$ false negative rate and zero false positive except the 5th client. The performance on different clients converges to the point with different iterations. It indicates that some clients are difficult to be recognised due to the variance between face images. Among these clients, AdaBoost has achieved high performance in early iterations. For the 1st and 4th client, after the 4th iteration, the $\mathcal{FP}$ is $25\%$ (only one positive example not being recognised) and the $\mathcal{FN}$ is $0.19\%$ (only $3$ out of $1400$ negative examples not being recognised). For the 5th client, after five iterations, the $\mathcal{FP}$ is $25\%$ and the $\mathcal{FN}$ is $0\%$. However, the performance on some clients (like the 2nd and 7th clients) is persistently low from the start to the end. These clients are ``hard'' clients in the \mbox{XM2VTS} face database due to large variance between face images within the client.

For both training set and testing set, the performance converges at a certain point at which the performance of classifiers gets stable. The number of iterations for the classifier being stable is much less than the one in the training.