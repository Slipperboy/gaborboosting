\chapter{Background}
\label{ch:back}
\section{Face Detection}
A first step for face recongition system is to detecting the location and the size of faces in images. Face detection is important and diffculty research topic in computer vision field because it is needed to slove some problems like varous positions, scales, in-plane rotation, orientation, pose and illumination. A general statement of face detection has been described in \cite{Ming2000}
\begin{quote}
Given an arbitary image, the goal of face detection is to detemine whether or not there are any faces in the image and, if present, return the image location and extent of each face.
\end{quote}

Face detection became a important research topic in computer vision and pattern recogntion field for more than 20 years. Face detection is the first step for a fully functioned automatic face recognition system. By face detection, faces are properly located and segemented into face region without background, hair upper body and so on. Later on, face identification or verification or other system can easily process the face region rather than a whole body of human being. Also face detection is a step of many visual surveillance systems\cite{Foresti2003}. Face locations returned by face detection are used to increase the detection accuracy and to avoid false detections due to occlusions or unfavorable human poses in detection system.  Face is a highly non-rigid object which is the most difficult issue for detection and recognition. Face detection provides techniques into modeling, estimation, curvature analysis, shape from shading, relational structures, model-based systems, and aspect graphs, which are useful to solve  non-rigid object problem in computer vision. Face detection is a step to generic object detection or recognition, because face detection is a typical two-class classification. The techniques used on face detection can applied to generic object detection or recognition. Finally, there are lots of face detection applications, such as face localization, face tracking, face finder and so on.

Face detection is one of visual tasks which human beings can do effortlessly. However, in computer vision terms face detection is a difficult problem. Pose is one of most important factors for face detection, in stills images, video sequences or realtime visvual surveillance system, human face may have different postures such as frontal face view, 45 degree face view (half profile), profile face view and upside down face view. 
Presence or absence of structural components, like beards, mustaches and glasses on faces also makes face detection a difficult task, because these components change the structure of face geometry dramaticly. Among these components, glasses are the issues influence the correctness of face detection since the the intensities in the eye regions are crucial for face detection \cite{Viola2001} and glasses damage the original structure of the intensities in the eye regions. Facial expression is also a key point deciding the robustness of face detection system. Face appearance is directly affected by a person's facial expression. If face appearance is changed, the detection system may failed. Also faces may be partially occluded by other objects when they are appeared in real-time system or real-world pictures. At last Imaging conditions may give good or bad credit to face detection. Lighting (spectra,source distribution and intensity), camera characteristics (sensor response, gain control, lenses) and resolution of image influnence face detection alot.

%Face detection is one of visual tasks which human beings can do effortlessly. However, in computer vision terms, this task is not easy. A general statement of the problem can be as follows: Given a still or video image, detect and localize an unknown number (if any) of faces \cite{Hjelms2001}. The solution to the problem involves image segmentation, feature extraction from an uncontrolled background. As a complete system, a face detection system should also be able to achieve the task regardless of illumination, orientation, and camera distance. Even for some strictly condition systems (such as face recognition, face verification and so on), there are standard input images. However they are “standard�to humans' eyes, but not standard to vision algorithms, because the algorithms have their own ways to understand the images rather than human’s ways. For some face images, we, human beings, think they are good candidates for the face recognition, but the algorithms “thinks" not.
There are some problems related to face detection. Face localization is a simplified version of face detection with the assumption that there is only one face contained in an input image. Face localiztion is to determine the image position of a single face. Facial feature extraction is to detect the presence and locate the positions of facial features such as eyes, nose, nostrils, eyebrow, mouth, lips. ears, etc. Like face localization, facial feature extraction also assumes that an input image contains only one single face. As the first step of completely face recognition system, face detection provides the input to face identification and facial expression recognition. In visual surveillance,face detection contributes to human pose estimation and tracking.

Face detection remains a two class classification problem - faces versus nonfaces. Faces are a category of certain highly non-rigid object. In the training stage, the systems need a training set, which includes face examples and nonface examples. Each face example is an image which contains only one face Every face example has the same size occupying the image regardless different individual. Nonface examples are images having the same format with face examples, however in the content of images are anything rather than human faces. Essentially, many methods in face detection can be applied to other object detection or reccognition by altering objects in the training examples.

The research issues on face detection include representation, scale, search strategy, speed, precision and post processing. Representation is a computational model used to represent typical faces. The computational model can be holistic face area, a sets of features, or edges and nodes. Scale means how to deal with faces with different sizes. Faces in an input image can not have unique size. The input image must be scaled into different common size to fit a window for detection. Search strategy decides the mechanism which is used to spot these faces in an image. Some common strategy are exhaustive, greedy, and focus of attention. Speed is also an important research issue because some face decetion adopt exhaustive search strategies. Precision is the most important issue which decides the success of failure of a face detection applicaiton. Post processing is to combine detection results, due to a same face can be resulted in different scales and different position.

Due to many reasons may influencing face detection, some contrains are given in the thesis to narrow down the topic of face detection. Faces in face detection are subjected to upright and frontal in a single gray scale image with decent resolution under proper lighting condition. The methods for face detection can be divided into four categories: Knowledge-based methods, Feature invariant approaches, Template matching approaches and Appearance-based methods. Knowledge-based methods are to encode human knowledge of what constitutes a typical face



\subsection{Earlier Approaches}
There are many algorithms proposed from simple edges grouping \cite{Sakai1972}\cite{Sirohey1993},\cite{Leung1995},\cite{Yow1997} to high level approaches utilizing a cascade structure of classifiers \cite{Viola2004}\cite{Freund1995}. Face detection requires a priori information of face. The information is the knowledge of face. The existing algorithms are categorized by the knowledge of face.  Therefore these algorithms presented in this section are divided as either feature-based or image-based, and are discussed in terms of their technical approach and performance. The algorithms in the first group utilize apparent features and follow the geometry-based \cite{Leung1995}, texture-based \cite{Yow1997}, or colour-based \cite{Chai1998} detection approaches. By taking the advantage of the benefit from current pattern recognition techniques, the algorithms in the second group represent faces as 2D intensity arrays, and address them as a general recognition problem.
Feature-based approaches
The apparent properties of face such as skin colour or face geometry are exploited in the feature-based approaches. Typically, in these approaches, face detection are accomplished by manipulating distance, angles, and area measurements of the visual features derived from the scene. Since features are the main ingredients, these techniques are called the feature-based approaches.

Grouping of Edges
As the most primitive feature in computer vision application, edges can represent the properties of face by applying edge detectors on images. Sakai et al. \cite{Sakai1972} analyze line drawing of the faces from photographs, and locate facial features.  These facial features like eyebrows, eyes, nose, mouth, and hair-line are commonly extracted. Based on the extracted features, a statistical model is built to describe their relationships and to verify the existence of a face. Sirohey \cite{Sirohey1993} proposed a localization method to segment a face from a cluttered background. It used an edge map generated by Canny detector, and heuristics to remove and group edges for preserving the ones on the face. An ellipse is then fit to the boundary between the head region and the background. Leung et al. \cite{Leung1995} developed a probabilistic method to locate a face in a cluttered scene. The method is to find the arrangement of certain facial features that is most likely to be a face pattern. These facial features are two eyes, two nostrils and nose/lip junction. Five features are used to describe a typical face. Given a testing image, candidate facial features are identified by matching the filter responses at each pixel against the typical face. Yow and Cipolla \cite{Yow1997} presented a feature-based method that uses a large amount of evidence from the visual image and their contextual evidence. The first stage applies a second derivative Gaussian filter to find the possible locations of facial features. The second stage examines the edges around these interest points and groups them into regions. The problem with these algorithms is that image features can be severely corrupted due to illumination, noise, and occlusion. Feature boundaries can be weakened for faces, while shadows can cause strong edges in maps. 

Texture or Skin colour
Human faces have a distinct texture that can be used to separate them from different object. Augusteijin and Skufca \cite{} transform the texture using second-order statistical features. The three types of features are considered: skin, hair and other. The textures are classified by using a neural network. The results suggest the occurrence of hair and skin textures. Dai and Nakano \cite{} extend this method. Using the face texture model, they design a scanning scheme for face detection in colour scenes. 

Human skin colour is a useful cue for face detection. It has been used and proven to be an effective feature in many applications from face detection to hand tracking. Many systems build a skin model and classify pixels on their colour. The simplest method \cite{} is to define a region of skin tone pixels using the values of Cr and Cb in YCrCb space. If the values fall within some ranges, the pixel is classified to have skin tone. Jones and Rehg \cite{} construct a histogram of RGB values due to skin pixels and a second histogram of RGB values due to non-skin pixels. These histograms serve as models of the class-conditional densities. Saxe and Foulds \cite{} proposed an iterative skin identification method that uses histogram intersection in HSV color space. Lee et al. \cite{} shows the HSI colour representation have advantages in giving large variance among facial feature colour clusters. In contrast to the nonparametric methods mentioned above, more complex methods make use of statistical measures that model face skin tone variance. Oliver et al. \cite{} employ a Gaussian distribution to represent a skin colour cluster of thousands of skin colour pixels. McKenna et al. \cite{} presented an adaptive Gaussian mixture model for colour to track faces under varying illumination conditions. The H-S model of a single person functions well with other races. The mixture model is used to assign a probability to each pixel in an image and faces are detected by grouping suitably sized areas of high probability.

Active Shape Models
Human faces can also be detected by using template matching. Template matching predefines or parameterizes a function, which depicts a typical face pattern. Given an input image, the correlation values with the typical pattern are computed for the face shape, eyes, nose and mouth independently. The occurrence of faces is determined by the correlation values. Kown and Lobo \cite{} developed a detection method based on snakes \cite{}. Each face is approximated by an ellipse which is found by applying a Hough transform \cite{}. The locations of faces are obtained by sets of four parameters describing ellipses. Lantitis et al. \cite{} described a face representation method with both shape and intensity information. They labelled the sampled contour such as the eye boundary, nose, chin/cheek in the training set, and used a vector of sample points to represent shape. The shape vector is characterized by a point distribution model (PDM). A face-shape PDM can be used to locate faces in new images by using active shape model (ASM) to estimate the face location and shape parameter. The averaging shape and intensity parameters can be used together for classification. Cootes and Taylor \cite{} applied this approach to localize a face in an image. In their work, Factor analysis \cite{} is applied to fit these training features and obtain a distribution function. 

\subsection{Modern Approaches}
Contrasted to face detection by explicit modelling of facial features, image-based representations of faces are directly classified into a face group. In general, image-based methods rely on techniques from statistical analysis and machine learning to find the relevant information of face and non-face images. This type of approaches eliminates the potential of modelling error, which is due to incomplete or inaccurate knowledge of faces. The basic method in detecting a face pattern is via a training procedure. The procedure classifies the incoming example into face or non-face prototype classes.

Most of the image-based approaches apply a window scanning techniques for detecting. The window scanning scheme is in essence an exhaustive search of the input image for possible face locations at all scales, but in the real applications there are variations in scales.

Eigenpictures based approaches
Kirby and Sirovich \cite{} developed a method to linearly encode faces by applying Karhuen-Loève transform \cite{}, which is also called principle component analysis (PCA). Given an ensemble different face images, the method first finds the principle components of the distribution of faces, expressed in terms of eigenvectors. Each individual face in the face set can be encoded as a linear combination of the top eigenvectors. Turk and Pentland \cite{} applied PCA to face recognition and face detection. By applied PCA on the training set of face images, eigenvectors span a subspace of the image face. This subspace is called face space. Images of faces are projected onto the subspace and clustered. To detect the presence of a face in a scene, the distance between an image region and the face space is computed for all locations in the image. The distance from face space is used as a measure of face existence, and the result of calculating the distance from face space is a “face map� A face can then be detected by setting a threshold for the face map. 

Distribution based approaches
For modelling the manifold of face images, PCA is not necessarily the best. Face space might be better represented by dividing it into subclasses. Sung and Poggio \cite{} developed a distribution-based system for face detection. In this system, the distribution of image patterns from one object class can be learned from positive and negative examples of that class. The system comprises two components, distribution-based models for face and non-face patterns and multilayer perception classifier. The patterns are grouped into six face and six non-face clusters using a modified k-means algorithm \cite{}. Each cluster is represented as multidimensional Gaussian distribution. Two distance metrics are computed. The first is the normalized Mahalanobis distance \cite{} between the test pattern and the cluster centroid. The second distance is the Euclidean distance \cite{} between the test pattern and its projection onto the face subspace. Multilayer perception network classifies face window patterns from non-face using twelve pairs of distances to each face and non-face cluster. 

Moghaddam and Pentland \cite{} have further developed the eigenpicture techniques within a probabilistic framework. When using PCA for representation, one normally discards the orthogonal complement of face space. It is found that face space has a uniform density. This method decomposes the vector space into two mutually exclusive and complementary subspaces: the principle subspace and its orthogonal complement. A maximum likelihood detector takes into account both face space and its orthogonal complement to handle arbitrary density.

Yang et al. \cite{} proposed two methods for face detection which also seek to represent the manifold of human faces as a set of subclass. The first method is based on Factor analysis (FA) \cite{}. FA is a statistical method for modelling the covariance structure of high dimensional data using a small number of latent variables. FA is similar to PCA, but in contrast to PCA, FA defines a proper density model. A mixture of factor analyzers is used in this method. Given a set of training images, an EM (Expectation Maximum) algorithm \cite{} is used to estimate the parameters in the mixture model of factor analyzers. This mixture model is then applied to sub-windows in the input image and outputs the probability of a face being present at the current location. The second method uses Fisher’s Linear Discriminate (FLD) \cite{} to project samples from the high dimensional image space to a lower dimensional feature space. 

Neural Networks
Neural network have been applied in many pattern recognition problems, including face detection. Neural network has its advantage of the feasibility of training a system to capture the complex class conditional density of face patterns. The disadvantage is that network has to be extensively tuned. It is very time consuming and to get exceptional performance. Among various face detection methods using neural networks, the most significant and typical work is done by Rowley et al. \cite{}. The system operates in two stages: it first applies a set of neural   network-based filters to an image, and then arbitrates the filter outputs. The neural network is designed to look at windows of 20×20 pixels. There is one hidden layer with 26 units, where 4 units with 10×10 pixel sub-regions, 16 with 5×5 pixel sub-regions, and 6 with 20×5 pixel overlapping horizontal stripes. The pre-processing first attempts to equalize the intensity values across the window. Then histogram equalization is performed, which compensates for differences in camera input gains, and improves the contrast in some cases. In each training image, the eyes, tip of the nose, corners, and centre of the mouth are labelled manually and used to normalize the face to the same scale, orientation, and position. Next step of this method is to merge overlapping detection and arbitrate between the outputs of multiple networks. Simple arbitration schemes such as logic operators (AND/OR) and voting are used to improve performance. One limitation of this method is that it only can detect upright, frontal faces. Rowel et al. \cite{} extended it to detect faces at any degree of rotation in the image plane. The system employs multiple networks; the first is a “router�network which processes each input window to determine its orientation and then uses this information to prepare the window for one or more detector networks.

Support Vector Machines
Support Vector Machine (SVM) \cite{} is a new pattern classification algorithm to train polynomial, neural network, or Radial Basis Functions classifiers. SVM is a simple, reliable, and effective technique, and it tends to be adopted quite widely. Other techniques used to train the above mentioned classifiers are based on the idea of minimizing the training error, which is usually called empirical risk. However, SVMs operate on another induction principle, called structural risk minimization, which minimizes an upper bound on the generalization error. SVM is equivalent to solving a linearly constrained Quadratic Programming problem in a number of variables equal to the number of data points, but has its drawbacks on both time and memory intensive. Osuna et al. \cite{} first applied SVM to face detection. In their work, an efficient method to train an SVM for large scale problems (above 10,000,000 samples) was developed. The system has slightly lower error rates and runs approximately 30 times faster than the system developed by Sung and Poggio \cite{}. Kumar and Poggio \cite{} incorporated SVM algorithms in a system for real-time tracking and faces analysis. SVMs are also applied on segmenting skin regions in the input images to avoid exhaustive scanning. In Terrillon et al.’s work \cite{}, SVMs improved the performance of the face detector compared to the earlier use of a multi-layer perception. 

Bayesian based approaches
Many image-based methods can be understood in a probabilistic framework. An image or feature vector derived from an image is viewed as a random variable x. The random variable is characterized for faces and non-faces by the class-conditional density functions p(x|face) and p(x|nonface). Schneiderman and Kanade \cite{} described a face detection method based on Bayes�decision rule. If the likelihood ratio between p(x|face) and p(x|nonface) is greater than the ratio between p(nonface) and p(face), then it is decided that a face is present at the current location. When the likelihoods are accurate, the Bayes decision rule is proven to be optimal. In their work, the face images were first normalized to 64×64 pixels. Second step is to decompose the face images into 16×16 sub-regions. Then the sub-regions are projected onto a 12-dimensional subspace constructed by PCA. Finally, the likelihood function is derived by normalizing the entire face region to have zero mean and unit variance. Later Schneiderman and Kanade extend this method with wavelet representations to detection profile faces and cars \cite{}.

Hidden Markov Modelss
By using the Hidden Markov Model (HMM) \cite{}, patterns can be characterized as a parametric random process and that the parameters of this process can be estimated in a precisely, well-defined manner. To apply HMMs for a pattern recognition problem, a number of hidden states need to be decided first to form a model. Once a model is defined, the HMM can learn the transitional probability between states from the examples where each example is represented as a sequence of observations. HMMs have been applied to both face recognition and localization by Samaria \cite{}. A face pattern is divided into several regions such as the forehead, eyes, nose, mouth, and chin. Then a process recognizes a face pattern in which these regions are observed in an appropriate order from top to bottom. This from top to bottom observation is taken as a block of pixels. For face patterns, the boundaries between strips of pixels are represented by probabilistic transitions between states. The image data within a region is modeled by a multivariate Gaussian distribution. An observation sequence consists of all intensity values from each block. The output states correspond to the face or non-face to which the observation belong. After the HMM has been trained, the output probability of an observation determines the class to which it belongs. 

AdaBoos approaches
In 2001, a new face detection approach proposed by Viola and Jones \cite{} is most clearly distinguished from above approaches. This approach has the extremely rapid rate for face detection. Their work is distinguished by three key contributions. The first contribution is a new image representation. Harr-like features are used so that they can be computed very rapidly at many scales. The second contribution is a method for construction a classifier by selecting a small number of important features using AdaBoost \cite{}. The third major contribution of is a method for combing successively more complex classifiers in a cascade structure which dramatically increases the speed of the detector by focusing attention on promising regions of the image. The cascaded classifier has the 38 layers. Each layer in the cascade is trained using the AdaBoost training procedure. The detector scanned across the image at the multiple scales and location. Scaling is achieved by scaling the detector itself. Operation on 384×288 pixel images, faces are detected at 15 frames per second on a conventional 700 MHz Intel Pentium III, which is taking 0.067 second per image. The algorithm has been implemented on a less powerful device �Compaq iPaq. It achieves two frames per second, which is taking 0.5 second on one image.

\section{Face Recognition}
\subsection{Appearance-based Face Recognition}
\subsubsection{Linear Methods}
\subparagraph{PCA}
\subparagraph{ICA}
\subparagraph{LDA}
\subparagraph{Others}
\subsubsection{Nonlinear Methods}
\subparagraph{Kernel PCA}
\subparagraph{ISO Map and LLE}
\subsection{Model-based Face Recognition}
\subsubsection{2D}
\subparagraph{Elastic Bunch Graph}
\subparagraph{Active Appearance Model}
\subsubsection{3D}
\subparagraph{3D Morphable Model}
\subsection{Other Schemes}
\subsection{Summary}
\section{Face Expression Recognition}
\section{Other applications}
\section{Summary}
